{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmI5Ce--xuxC"
      },
      "source": [
        "# **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4xQWeD1tuSX"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.initializers import *\n",
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.python.framework.random_seed import set_random_seed\n",
        "\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "\n",
        "from PIL import Image, ImageEnhance, ImageFilter\n",
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoy2nf3FxXVl"
      },
      "source": [
        "# **Load Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qprXTMnut41p"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_cat = to_categorical(y_test)\n",
        "x_test_ready = normalize(flatten(x_test))"
      ],
      "metadata": {
        "id": "acIigEq4r-nS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbXBR7-P2TxX"
      },
      "outputs": [],
      "source": [
        "def show_img(pos, X=x_train, cmap='gray'):\n",
        "  plt.imshow(X[pos], cmap=cmap)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOj0vmfgHN6_"
      },
      "source": [
        "**Convert array to images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOv-vAxp6urr"
      },
      "outputs": [],
      "source": [
        "def ndarray_to_PILIM(X):\n",
        "  images = []\n",
        "  for i in range(len(X)):\n",
        "    images.append(Image.fromarray(X[i]))\n",
        "  return images\n",
        "\n",
        "def PILIM_to_ndarray(X):\n",
        "  arrays = []\n",
        "  for i in range(len(X)):\n",
        "    arrays.append(np.asarray(X[i]))\n",
        "  return np.array(arrays)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D06Ws39pxmZi"
      },
      "source": [
        "# **Preprocessing and Data Augmentation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vibwNMWAMuqY"
      },
      "source": [
        "**Preprocessing and Data Augmentation**\n",
        "\n",
        "\n",
        "1.   flattening\n",
        "2.   contrast\n",
        "3.   rotation\n",
        "4.   normalization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiKH8gbY0j0b"
      },
      "source": [
        "images transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4yhUn26HtvX"
      },
      "outputs": [],
      "source": [
        "def flatten(X):\n",
        "  x_flat = np.zeros((X.shape[0],X[0].flatten().shape[0]))\n",
        "  for i in range(X.shape[0]):\n",
        "    x_flat[i] = X[i].flatten()\n",
        "  return x_flat\n",
        "\n",
        "def contrast(X, factor=1.5):\n",
        "  out = []\n",
        "  for im in X:\n",
        "    enhancer = ImageEnhance.Contrast(im)\n",
        "    res = enhancer.enhance(factor)\n",
        "    out.append(res)\n",
        "  return out\n",
        "\n",
        "def rotate_img(image, angle=90):\n",
        "  return image.rotate(angle)\n",
        "\n",
        "def rotate(X, angle=90):\n",
        "  x_rotated = list(map(rotate_img, X))\n",
        "  return x_rotated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtqffU5T0sj5"
      },
      "source": [
        "numerical transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spS7dBTw0sHq"
      },
      "outputs": [],
      "source": [
        "def normalize(X):\n",
        "  x_norm = X/255\n",
        "  return x_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSyUeGgxh2zs"
      },
      "outputs": [],
      "source": [
        "im_fun = ['contrast']\n",
        "num_fun = ['normalize']\n",
        "\n",
        "def preprocessing(X, y, im_fun=im_fun, num_fun=num_fun):\n",
        "  out = ndarray_to_PILIM(X)\n",
        "  x_images = ndarray_to_PILIM(X)\n",
        "  if im_fun:\n",
        "    for f in im_fun:\n",
        "      out = out + globals()[f](x_images)\n",
        "      y = np.concatenate((y, y))\n",
        "  out = PILIM_to_ndarray(out)\n",
        "  if num_fun:\n",
        "    for f in num_fun:\n",
        "      out = globals()[f](out)\n",
        "  return flatten(out),y\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ42qSNjolJM"
      },
      "outputs": [],
      "source": [
        "\n",
        "x_train_ready = normalize(flatten(x_train))\n",
        "y_train_cat = to_categorical(y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_ready, x_val, y_train_cat, y_val = train_test_split(\n",
        "    x_train_ready, y_train_cat, test_size=0.2, shuffle=True,\n",
        "    random_state=50\n",
        ")"
      ],
      "metadata": {
        "id": "SM_oImV1DNAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxA9KQaShFU8",
        "outputId": "9f2c0c68-501f-4c6a-96fd-3894fc96c2be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlpnzw5uzQ4K"
      },
      "source": [
        "# **Models**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Returns a compiled model\n",
        "- n_units: [list] number of neurons for each hidden layer\n",
        "- activation_fs: [list] activation function for each hidden layer\n",
        "- opt: the optimizer\n",
        "- loss_f: the loss function. use \"categorical_crossentropy\" with one-hot encoding, and tf.keras.losses.SparseCategoricalCrossentropy() with KFold\n",
        "\n",
        "nb: the size of n_units, activation_fs must be the same\n",
        "\"\"\"\n",
        "def MLP_definer(n_units, activation_fs, opt, loss_f):\n",
        "\n",
        "  np.random.seed(50)\n",
        "  set_random_seed(50)\n",
        " \n",
        "  model = Sequential()\n",
        "  model.add(Dense\n",
        "            (input_dim = n_features, \n",
        "              units = n_features, \n",
        "              activation=\"relu\"))\n",
        "   \n",
        "  for i in range(len(n_units)):\n",
        "    model.add(Dense(\n",
        "        units = n_units[i],\n",
        "        activation = activation_fs[i],\n",
        "        kernel_initializer = \"random_uniform\"))\n",
        "\n",
        "  model.add(Dense(units = num_classes, activation='softmax')) #output layer\n",
        "\n",
        "  model.compile(loss = loss_f, #tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "      optimizer = opt,  #optimizer\n",
        "      metrics = ['accuracy']) #displayed metric\n",
        "\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "The same function as above but we add a Dropout layer after each Dense \n",
        "- drop_prob: probability for the dropout layer (es. 0.30)\n",
        "'''\n",
        "\n",
        "def MLP_definer_drop(n_units, activation_fs, opt, drop_prob, loss_f):\n",
        "\n",
        "  np.random.seed(50)\n",
        "  set_random_seed(50)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Dense\n",
        "            (input_dim = n_features, \n",
        "              units = n_features, \n",
        "              activation=\"relu\"))\n",
        "   \n",
        "  for i in range(len(n_units)):\n",
        "    model.add(Dense(\n",
        "        units = n_units[i],\n",
        "        activation = activation_fs[i]))\n",
        "    \n",
        "    model.add(Dropout(drop_prob))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "  model.add(Dense(units = num_classes, activation='softmax')) #output layer\n",
        "\n",
        "  model.compile(loss = loss_f, #tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "      optimizer = opt,  #optimizer\n",
        "      metrics = ['accuracy']) #displayed metric\n",
        "\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "rh4eAuznY3Cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpNS8ayrjUgc"
      },
      "outputs": [],
      "source": [
        "def layer_builder(units, activation_f='relu', drop=False, batch_n=False, initializer='random_normal'):\n",
        "  layer = Dense(units, activation = activation_f, kernel_initializer = initializer)\n",
        "  out=[layer]\n",
        "\n",
        "  if drop:\n",
        "    drop_l = Dropout(drop)\n",
        "    out.append(drop_l)\n",
        "  if batch_n:\n",
        "    batch_l = BatchNormalization()\n",
        "    out.append(batch_l)\n",
        "\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYmrjnpxpjVF"
      },
      "outputs": [],
      "source": [
        "def MLP_definer(layers, opt='sgd', loss_f='categorical_crossentropy', metrics=['accuracy'], n_class=10, n_features=784):\n",
        "  np.random.seed(50)\n",
        "  set_random_seed(50)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Dense(input_dim=n_features, units=n_features, activation='relu'))\n",
        "  \n",
        "  for i in range(len(layers)-1):\n",
        "    for l in layers[i]:\n",
        "      model.add(l)\n",
        "      \n",
        "  model.add(layers[-1])\n",
        "  \n",
        "  model.compile(loss=loss_f, optimizer=opt, metrics=metrics)\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zOFMW7NsbZc"
      },
      "outputs": [],
      "source": [
        "layer_dict = {'units':[512,256,10],\n",
        "              'activation_f':'relu',\n",
        "              'drop':0.5,\n",
        "              'batch_n':False,\n",
        "              'kernel_initializer':'glorot_uniform'}\n",
        "\n",
        "model_dict = {'opt':SGD(),\n",
        "              'loss_f':'categorical_crossentropy',\n",
        "              'metrics':['accuracy'],\n",
        "              'n_features':784}\n",
        "\n",
        "\n",
        "\n",
        "def MLP_builder(l_dict, m_dict):\n",
        "  layers=[]\n",
        "\n",
        "  for i in range(len(l_dict['units'])-1):\n",
        "    new_l = layer_builder(l_dict['units'][i],\n",
        "                          activation_f=l_dict['activation_f'],\n",
        "                          drop=l_dict['drop'],\n",
        "                          batch_n=l_dict['batch_n'],\n",
        "                          initializer=l_dict['kernel_initializer'])\n",
        "    layers.append(new_l)\n",
        "\n",
        "  layers.append(Dense(l_dict['units'][-1],\n",
        "                      activation='softmax'))\n",
        "  \n",
        "\n",
        "  model = MLP_definer(layers,\n",
        "                      opt=m_dict['opt'],\n",
        "                      loss_f=m_dict['loss_f'],\n",
        "                      metrics=m_dict['metrics'],\n",
        "                      n_features=m_dict['n_features'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKsdOogM9wRh"
      },
      "outputs": [],
      "source": [
        "def MLPs_builder(l_dict, m_dict):\n",
        "  models = []\n",
        "\n",
        "  for e in range(len(l_dict['activation_f'])):\n",
        "    for p in range(len(l_dict['batch_n'])):\n",
        "      for d in range(len(l_dict['drop'])):\n",
        "        for k in range(len(l_dict['kernel_initializer'])):\n",
        "          for ls_units in l_dict['units']:\n",
        "            layersa = []\n",
        "            for i in range(len(ls_units)-1):\n",
        "              new_l = layer_builder(units=ls_units[i],\n",
        "                                    initializer = l_dict['kernel_initializer'][k],\n",
        "                                    activation_f = l_dict['activation_f'][e],\n",
        "                                    drop = l_dict['drop'][d],\n",
        "                                    batch_n = l_dict['batch_n'][p])\n",
        "              layersa.append(new_l)\n",
        "            \n",
        "            layersa.append(Dense(ls_units[-1],\n",
        "                            activation='softmax'))\n",
        "\n",
        "            for o in range(len(m_dict['opt'])):\n",
        "              model = MLP_definer(layers = layersa,\n",
        "                                  opt = m_dict['opt'][o],\n",
        "                                  loss_f = m_dict['loss_f'],\n",
        "                                  n_features = m_dict['n_features'])\n",
        "\n",
        "              models.append(model)\n",
        "\n",
        "  return models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcAB9HVO4FOq"
      },
      "source": [
        "# **Tests**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_NmXRT14LVc"
      },
      "source": [
        "Test funzioni costruzione modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zn5ptrUuCJbG"
      },
      "outputs": [],
      "source": [
        "def train_models(model_list, x, y, x_val, y_val, min_acc=0.7, epochs=50):\n",
        "  hst=[]\n",
        "  for model in model_list:\n",
        "    print('Training model {}/{}'.format(model_list.index(model)+1, len(model_list)))\n",
        "    model_hist = model.fit(x, y,\n",
        "                           epochs=epochs,\n",
        "                           batch_size=256,\n",
        "                           validation_data=(x_val, y_val),\n",
        "                           verbose=1)\n",
        "    best_epoch = get_best_epoch(model_hist.history)\n",
        "    if best_epoch['val_accuracy'] > min_acc:\n",
        "      hst.append(model_hist)\n",
        "\n",
        "  return hst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxqb8adsHQuF"
      },
      "outputs": [],
      "source": [
        "def get_best_epoch(diz, watch='val_accuracy'):\n",
        "\n",
        "  if(watch == \"val_accuracy\" or watch == \"accuracy\"):\n",
        "\n",
        "    if(watch == \"val_accuracy\"):\n",
        "      ls = diz.get(\"val_accuracy\")\n",
        "      acc_val_max = max(ls)\n",
        "      ind = ls.index(acc_val_max)\n",
        "\n",
        "      ls2 = diz.get(\"accuracy\")\n",
        "      acc_max = ls2[ind]\n",
        "\n",
        "    elif(watch == \"accuracy\"):\n",
        "      ls = diz.get(\"accuracy\")\n",
        "      acc_max = max(ls)\n",
        "      ind = ls.index(acc_max)\n",
        "\n",
        "      ls2 = diz.get(\"val_accuracy\")\n",
        "      acc_val_max = ls2[ind]\n",
        "\n",
        "    ls1 = diz.get(\"loss\")\n",
        "    loss_ = ls1[ind]\n",
        "\n",
        "    ls3 = diz.get(\"val_loss\")\n",
        "    loss_val_ = ls3[ind]\n",
        "\n",
        "    out = {\n",
        "        \"accuracy\": acc_max,\n",
        "        \"loss\": loss_,\n",
        "        \"val_accuracy\": acc_val_max,\n",
        "        \"val_loss\": loss_val_,\n",
        "        \"ind\": ind\n",
        "        }\n",
        "\n",
        "    return out\n",
        "\n",
        "  else:\n",
        "    print(\"only val_accuracy or accuracy are suitable params\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPgnJpnKI34J"
      },
      "outputs": [],
      "source": [
        "def print_hist(hist_list):\n",
        "\n",
        "  optimizers = [SGD, Adam]\n",
        "  opt_names = ['SGD', 'Adam']\n",
        "  initilizers = [GlorotUniform, GlorotNormal, RandomUniform, RandomNormal]\n",
        "  init_names = ['glorot_uniform', 'glorot_normal', 'random_uniform', 'random_normal']\n",
        "\n",
        "  for hst in hist_list:\n",
        "    best = get_best_epoch(hst.history)\n",
        "    print(\"============================================================================\")\n",
        "\n",
        "    units=[]\n",
        "    for l in hst.model.layers:\n",
        "      try:\n",
        "        units.append(l.units)\n",
        "      except:\n",
        "        pass\n",
        "    print('UNITS: ',units)\n",
        "\n",
        "    for k in range(len(initilizers)):\n",
        "      if isinstance(hst.model.layers[1].kernel_initializer, initilizers[k]):\n",
        "        print(\"INIT: \",init_names[k], end=\" - \")\n",
        "        \n",
        "    for i in range(len(optimizers)):\n",
        "      if isinstance(hst.model.optimizer, optimizers[i]):\n",
        "        print(\"OPT: {0}(lr={1:.3f})\".format(opt_names[i],float(hst.model.optimizer.learning_rate.value())), end=\" - \")\n",
        "    \n",
        "    #il primo layer di drop (se esiste) ha indice 2. (input, primo dense, drop)\n",
        "    if isinstance(hst.model.layers[2], Dropout):\n",
        "      print(\"DROP: \",hst.model.layers[2].rate, end=\" - \")\n",
        "    else:\n",
        "      print(\"DROP: False\", end=\" - \")\n",
        "    \n",
        "    #il primo layer di batch normalization sta in posizione 2 o 3\n",
        "    #a seconda che venga usato il drop o meno\n",
        "    if isinstance(hst.model.layers[2], BatchNormalization):\n",
        "      print(\"BATCH_NORM: True\")\n",
        "    else:\n",
        "      if len(hst.model.layers) > 3 and isinstance(hst.model.layers[3], BatchNormalization):\n",
        "        print(\"BATCH_NORM: True\")\n",
        "      else:\n",
        "        print(\"BATCH_NORM: False\")\n",
        "    \n",
        "    print(\"LAST VAL_ACC: \", hst.history[\"val_accuracy\"][-1])\n",
        "    print(\"LAST VAL_LOSS: \", hst.history[\"val_loss\"][-1])\n",
        "    print(\"BEST: \", best)\n",
        "\n",
        "    plot_history(hst)\n",
        "\n",
        "    print(\"============================================================================\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(histo):\n",
        "\n",
        "  plt.plot(histo.history['accuracy'])\n",
        "  plt.plot(histo.history['val_accuracy'])\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['accuracy', 'val_accuracy'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(histo.history['loss'])\n",
        "  plt.plot(histo.history['val_loss'])\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['accuracy', 'val_accuracy'], loc='upper left')\n",
        "  plt.show()\n",
        " "
      ],
      "metadata": {
        "id": "IkBxqUZigSI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOvAL9CIoZgV"
      },
      "outputs": [],
      "source": [
        "opt_list = [SGD()]\n",
        "layer_dict = {'units':[[512,256,128,64,32,30,20,10]],\n",
        "              'activation_f': ['relu'],\n",
        "              'drop': [False], \n",
        "              'batch_n': [False],\n",
        "              'kernel_initializer': ['random_normal'] }\n",
        "\n",
        "\n",
        "model_dict = {'opt': opt_list,\n",
        "              'loss_f':'categorical_crossentropy',\n",
        "              'metrics': ['accuracy'],\n",
        "              'n_features': 784}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5S6DCo3ozrk",
        "outputId": "df84eed8-c37a-4f9d-8049-8b0f522847b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "model_list = MLPs_builder(layer_dict, model_dict)\n",
        "print(len(model_list))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_list[0].summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqeH43YiS9Sx",
        "outputId": "3bfbe046-b35c-42a4-f95c-fb2a745d9e09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_7 (Dense)             (None, 784)               615440    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               401920    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 16)                528       \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 10)                170       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,192,618\n",
            "Trainable params: 1,192,618\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJ4oOnB4Gnx7",
        "outputId": "cf56edc7-6ab7-42d8-ec35-bf6894dc7bb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model 1/1\n",
            "Epoch 1/200\n",
            "188/188 [==============================] - 2s 10ms/step - loss: 2.3026 - accuracy: 0.0987 - val_loss: 2.3026 - val_accuracy: 0.0973\n",
            "Epoch 2/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 2.3025 - accuracy: 0.1036 - val_loss: 2.3026 - val_accuracy: 0.0991\n",
            "Epoch 3/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 2.3025 - accuracy: 0.1020 - val_loss: 2.3025 - val_accuracy: 0.0959\n",
            "Epoch 4/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 2.3025 - accuracy: 0.1010 - val_loss: 2.3025 - val_accuracy: 0.1187\n",
            "Epoch 5/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 2.3024 - accuracy: 0.1048 - val_loss: 2.3025 - val_accuracy: 0.1199\n",
            "Epoch 6/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 2.3024 - accuracy: 0.1090 - val_loss: 2.3025 - val_accuracy: 0.0993\n",
            "Epoch 7/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 2.3024 - accuracy: 0.1123 - val_loss: 2.3025 - val_accuracy: 0.0959\n",
            "Epoch 8/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 2.3024 - accuracy: 0.1026 - val_loss: 2.3025 - val_accuracy: 0.0959\n",
            "Epoch 9/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 2.3023 - accuracy: 0.1010 - val_loss: 2.3024 - val_accuracy: 0.0959\n",
            "Epoch 10/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 2.3023 - accuracy: 0.1010 - val_loss: 2.3024 - val_accuracy: 0.0959\n",
            "Epoch 11/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 2.3022 - accuracy: 0.1011 - val_loss: 2.3023 - val_accuracy: 0.0959\n",
            "Epoch 12/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 2.3022 - accuracy: 0.1010 - val_loss: 2.3023 - val_accuracy: 0.0959\n",
            "Epoch 13/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 2.3021 - accuracy: 0.1009 - val_loss: 2.3022 - val_accuracy: 0.0959\n",
            "Epoch 14/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 2.3020 - accuracy: 0.1096 - val_loss: 2.3021 - val_accuracy: 0.0959\n",
            "Epoch 15/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 2.3020 - accuracy: 0.1014 - val_loss: 2.3021 - val_accuracy: 0.0959\n",
            "Epoch 16/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 2.3019 - accuracy: 0.1010 - val_loss: 2.3020 - val_accuracy: 0.0963\n",
            "Epoch 17/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 2.3018 - accuracy: 0.1030 - val_loss: 2.3019 - val_accuracy: 0.0991\n",
            "Epoch 18/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 2.3017 - accuracy: 0.1130 - val_loss: 2.3017 - val_accuracy: 0.0966\n",
            "Epoch 19/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 2.3015 - accuracy: 0.1034 - val_loss: 2.3016 - val_accuracy: 0.1088\n",
            "Epoch 20/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 2.3014 - accuracy: 0.1031 - val_loss: 2.3014 - val_accuracy: 0.1283\n",
            "Epoch 21/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 2.3011 - accuracy: 0.1133 - val_loss: 2.3012 - val_accuracy: 0.1272\n",
            "Epoch 22/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 2.3009 - accuracy: 0.1178 - val_loss: 2.3008 - val_accuracy: 0.1232\n",
            "Epoch 23/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 2.3005 - accuracy: 0.1192 - val_loss: 2.3004 - val_accuracy: 0.1192\n",
            "Epoch 24/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 2.2999 - accuracy: 0.1280 - val_loss: 2.2997 - val_accuracy: 0.1173\n",
            "Epoch 25/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 2.2991 - accuracy: 0.1229 - val_loss: 2.2987 - val_accuracy: 0.1171\n",
            "Epoch 26/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 2.2978 - accuracy: 0.1262 - val_loss: 2.2971 - val_accuracy: 0.1563\n",
            "Epoch 27/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 2.2957 - accuracy: 0.1651 - val_loss: 2.2944 - val_accuracy: 0.1918\n",
            "Epoch 28/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 2.2919 - accuracy: 0.1959 - val_loss: 2.2891 - val_accuracy: 0.1721\n",
            "Epoch 29/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 2.2837 - accuracy: 0.1834 - val_loss: 2.2765 - val_accuracy: 0.1838\n",
            "Epoch 30/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 2.2603 - accuracy: 0.1894 - val_loss: 2.2352 - val_accuracy: 0.2045\n",
            "Epoch 31/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 2.1735 - accuracy: 0.2013 - val_loss: 2.0946 - val_accuracy: 0.2068\n",
            "Epoch 32/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 2.0176 - accuracy: 0.2074 - val_loss: 1.9552 - val_accuracy: 0.2128\n",
            "Epoch 33/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 1.9123 - accuracy: 0.2131 - val_loss: 1.8701 - val_accuracy: 0.2204\n",
            "Epoch 34/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 1.8310 - accuracy: 0.2212 - val_loss: 1.7842 - val_accuracy: 0.2282\n",
            "Epoch 35/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 1.7237 - accuracy: 0.2547 - val_loss: 1.6307 - val_accuracy: 0.2993\n",
            "Epoch 36/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 1.4587 - accuracy: 0.3923 - val_loss: 1.2708 - val_accuracy: 0.4643\n",
            "Epoch 37/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 1.2988 - accuracy: 0.4284 - val_loss: 1.4498 - val_accuracy: 0.4086\n",
            "Epoch 38/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 1.1735 - accuracy: 0.5017 - val_loss: 1.3028 - val_accuracy: 0.5120\n",
            "Epoch 39/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 1.1234 - accuracy: 0.5527 - val_loss: 1.1270 - val_accuracy: 0.5747\n",
            "Epoch 40/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 1.0908 - accuracy: 0.5874 - val_loss: 0.9726 - val_accuracy: 0.6604\n",
            "Epoch 41/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 1.0223 - accuracy: 0.6191 - val_loss: 0.9440 - val_accuracy: 0.6563\n",
            "Epoch 42/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.9742 - accuracy: 0.6406 - val_loss: 1.0888 - val_accuracy: 0.5771\n",
            "Epoch 43/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.9498 - accuracy: 0.6504 - val_loss: 0.9535 - val_accuracy: 0.6394\n",
            "Epoch 44/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.9220 - accuracy: 0.6539 - val_loss: 0.8209 - val_accuracy: 0.6984\n",
            "Epoch 45/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 0.8579 - accuracy: 0.6765 - val_loss: 0.8650 - val_accuracy: 0.6678\n",
            "Epoch 46/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.8450 - accuracy: 0.6844 - val_loss: 0.7967 - val_accuracy: 0.7057\n",
            "Epoch 47/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 0.8190 - accuracy: 0.6899 - val_loss: 0.8405 - val_accuracy: 0.6306\n",
            "Epoch 48/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.7752 - accuracy: 0.7038 - val_loss: 0.9104 - val_accuracy: 0.6458\n",
            "Epoch 49/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 0.7828 - accuracy: 0.7018 - val_loss: 0.7744 - val_accuracy: 0.7167\n",
            "Epoch 50/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 0.7290 - accuracy: 0.7200 - val_loss: 0.7338 - val_accuracy: 0.7075\n",
            "Epoch 51/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 0.7153 - accuracy: 0.7281 - val_loss: 1.0271 - val_accuracy: 0.6522\n",
            "Epoch 52/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 0.7090 - accuracy: 0.7337 - val_loss: 0.6786 - val_accuracy: 0.7197\n",
            "Epoch 53/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.6788 - accuracy: 0.7478 - val_loss: 0.6278 - val_accuracy: 0.7812\n",
            "Epoch 54/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.6827 - accuracy: 0.7442 - val_loss: 0.6192 - val_accuracy: 0.7871\n",
            "Epoch 55/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.6634 - accuracy: 0.7608 - val_loss: 0.6109 - val_accuracy: 0.7878\n",
            "Epoch 56/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.6199 - accuracy: 0.7745 - val_loss: 0.6072 - val_accuracy: 0.7891\n",
            "Epoch 57/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 0.6061 - accuracy: 0.7792 - val_loss: 0.6033 - val_accuracy: 0.7817\n",
            "Epoch 58/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.6074 - accuracy: 0.7781 - val_loss: 0.6133 - val_accuracy: 0.7743\n",
            "Epoch 59/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 0.5688 - accuracy: 0.7955 - val_loss: 0.7349 - val_accuracy: 0.7432\n",
            "Epoch 60/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.5710 - accuracy: 0.7939 - val_loss: 0.5510 - val_accuracy: 0.8088\n",
            "Epoch 61/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 0.5565 - accuracy: 0.8026 - val_loss: 0.6069 - val_accuracy: 0.7772\n",
            "Epoch 62/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.5462 - accuracy: 0.8068 - val_loss: 0.5488 - val_accuracy: 0.8043\n",
            "Epoch 63/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 0.5277 - accuracy: 0.8121 - val_loss: 0.5332 - val_accuracy: 0.8172\n",
            "Epoch 64/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 0.5308 - accuracy: 0.8128 - val_loss: 0.5091 - val_accuracy: 0.8271\n",
            "Epoch 65/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.5003 - accuracy: 0.8245 - val_loss: 0.5510 - val_accuracy: 0.7917\n",
            "Epoch 66/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.4994 - accuracy: 0.8245 - val_loss: 0.5524 - val_accuracy: 0.8099\n",
            "Epoch 67/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 0.4958 - accuracy: 0.8260 - val_loss: 0.5465 - val_accuracy: 0.8028\n",
            "Epoch 68/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.4843 - accuracy: 0.8310 - val_loss: 0.5287 - val_accuracy: 0.8268\n",
            "Epoch 69/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 0.4996 - accuracy: 0.8263 - val_loss: 0.6928 - val_accuracy: 0.7448\n",
            "Epoch 70/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.4576 - accuracy: 0.8423 - val_loss: 0.6005 - val_accuracy: 0.7835\n",
            "Epoch 71/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.4667 - accuracy: 0.8383 - val_loss: 0.5110 - val_accuracy: 0.8257\n",
            "Epoch 72/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.4485 - accuracy: 0.8463 - val_loss: 0.5101 - val_accuracy: 0.8278\n",
            "Epoch 73/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.4707 - accuracy: 0.8366 - val_loss: 0.5343 - val_accuracy: 0.8184\n",
            "Epoch 74/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.4335 - accuracy: 0.8500 - val_loss: 0.4784 - val_accuracy: 0.8390\n",
            "Epoch 75/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.4569 - accuracy: 0.8423 - val_loss: 0.4759 - val_accuracy: 0.8444\n",
            "Epoch 76/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.4148 - accuracy: 0.8580 - val_loss: 0.4976 - val_accuracy: 0.8206\n",
            "Epoch 77/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 0.4151 - accuracy: 0.8564 - val_loss: 0.4766 - val_accuracy: 0.8401\n",
            "Epoch 78/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.4093 - accuracy: 0.8584 - val_loss: 0.6146 - val_accuracy: 0.7853\n",
            "Epoch 79/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.4293 - accuracy: 0.8501 - val_loss: 0.4492 - val_accuracy: 0.8499\n",
            "Epoch 80/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 0.4683 - accuracy: 0.8424 - val_loss: 0.4501 - val_accuracy: 0.8515\n",
            "Epoch 81/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.4039 - accuracy: 0.8610 - val_loss: 0.4409 - val_accuracy: 0.8557\n",
            "Epoch 82/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.3846 - accuracy: 0.8679 - val_loss: 0.4428 - val_accuracy: 0.8557\n",
            "Epoch 83/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.3776 - accuracy: 0.8698 - val_loss: 0.4343 - val_accuracy: 0.8572\n",
            "Epoch 84/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.4373 - accuracy: 0.8531 - val_loss: 0.5368 - val_accuracy: 0.8296\n",
            "Epoch 85/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.3749 - accuracy: 0.8727 - val_loss: 0.4303 - val_accuracy: 0.8597\n",
            "Epoch 86/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.3666 - accuracy: 0.8738 - val_loss: 0.4419 - val_accuracy: 0.8543\n",
            "Epoch 87/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.3621 - accuracy: 0.8764 - val_loss: 0.4748 - val_accuracy: 0.8400\n",
            "Epoch 88/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.3499 - accuracy: 0.8812 - val_loss: 0.4392 - val_accuracy: 0.8543\n",
            "Epoch 89/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.3504 - accuracy: 0.8802 - val_loss: 0.4479 - val_accuracy: 0.8509\n",
            "Epoch 90/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.3391 - accuracy: 0.8836 - val_loss: 0.4480 - val_accuracy: 0.8560\n",
            "Epoch 91/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 0.3434 - accuracy: 0.8813 - val_loss: 0.4186 - val_accuracy: 0.8633\n",
            "Epoch 92/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.3268 - accuracy: 0.8863 - val_loss: 0.4153 - val_accuracy: 0.8673\n",
            "Epoch 93/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.3321 - accuracy: 0.8848 - val_loss: 0.4518 - val_accuracy: 0.8522\n",
            "Epoch 94/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.3228 - accuracy: 0.8881 - val_loss: 0.5668 - val_accuracy: 0.8223\n",
            "Epoch 95/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.3298 - accuracy: 0.8857 - val_loss: 0.4501 - val_accuracy: 0.8581\n",
            "Epoch 96/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.3123 - accuracy: 0.8912 - val_loss: 0.4410 - val_accuracy: 0.8569\n",
            "Epoch 97/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.3080 - accuracy: 0.8931 - val_loss: 0.4281 - val_accuracy: 0.8644\n",
            "Epoch 98/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.3090 - accuracy: 0.8926 - val_loss: 0.5510 - val_accuracy: 0.8341\n",
            "Epoch 99/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2947 - accuracy: 0.8970 - val_loss: 0.4186 - val_accuracy: 0.8666\n",
            "Epoch 100/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2850 - accuracy: 0.9018 - val_loss: 0.4178 - val_accuracy: 0.8701\n",
            "Epoch 101/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.7034 - accuracy: 0.7555 - val_loss: 0.4346 - val_accuracy: 0.8611\n",
            "Epoch 102/200\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 0.3062 - accuracy: 0.8954 - val_loss: 0.4298 - val_accuracy: 0.8568\n",
            "Epoch 103/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2814 - accuracy: 0.9023 - val_loss: 0.4022 - val_accuracy: 0.8733\n",
            "Epoch 104/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2733 - accuracy: 0.9044 - val_loss: 0.4684 - val_accuracy: 0.8491\n",
            "Epoch 105/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2896 - accuracy: 0.8993 - val_loss: 0.4272 - val_accuracy: 0.8700\n",
            "Epoch 106/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2657 - accuracy: 0.9063 - val_loss: 0.4112 - val_accuracy: 0.8752\n",
            "Epoch 107/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2627 - accuracy: 0.9084 - val_loss: 0.4611 - val_accuracy: 0.8633\n",
            "Epoch 108/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2582 - accuracy: 0.9099 - val_loss: 0.4381 - val_accuracy: 0.8644\n",
            "Epoch 109/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.3182 - accuracy: 0.8953 - val_loss: 0.4482 - val_accuracy: 0.8565\n",
            "Epoch 110/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2669 - accuracy: 0.9071 - val_loss: 0.4203 - val_accuracy: 0.8724\n",
            "Epoch 111/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2453 - accuracy: 0.9136 - val_loss: 0.4416 - val_accuracy: 0.8599\n",
            "Epoch 112/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2376 - accuracy: 0.9161 - val_loss: 0.4797 - val_accuracy: 0.8627\n",
            "Epoch 113/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2464 - accuracy: 0.9130 - val_loss: 0.4259 - val_accuracy: 0.8715\n",
            "Epoch 114/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2455 - accuracy: 0.9127 - val_loss: 0.5322 - val_accuracy: 0.8367\n",
            "Epoch 115/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2348 - accuracy: 0.9179 - val_loss: 0.5003 - val_accuracy: 0.8522\n",
            "Epoch 116/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.3448 - accuracy: 0.8890 - val_loss: 0.3966 - val_accuracy: 0.8766\n",
            "Epoch 117/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2170 - accuracy: 0.9245 - val_loss: 0.4086 - val_accuracy: 0.8723\n",
            "Epoch 118/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.3144 - accuracy: 0.8985 - val_loss: 0.4068 - val_accuracy: 0.8728\n",
            "Epoch 119/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2108 - accuracy: 0.9254 - val_loss: 0.4231 - val_accuracy: 0.8658\n",
            "Epoch 120/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2101 - accuracy: 0.9260 - val_loss: 0.4491 - val_accuracy: 0.8649\n",
            "Epoch 121/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2095 - accuracy: 0.9265 - val_loss: 0.4754 - val_accuracy: 0.8586\n",
            "Epoch 122/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.5750 - accuracy: 0.8021 - val_loss: 0.7628 - val_accuracy: 0.7045\n",
            "Epoch 123/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.3336 - accuracy: 0.8819 - val_loss: 0.4085 - val_accuracy: 0.8746\n",
            "Epoch 124/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2103 - accuracy: 0.9264 - val_loss: 0.4250 - val_accuracy: 0.8735\n",
            "Epoch 125/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1972 - accuracy: 0.9311 - val_loss: 0.4061 - val_accuracy: 0.8736\n",
            "Epoch 126/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1979 - accuracy: 0.9311 - val_loss: 0.4310 - val_accuracy: 0.8748\n",
            "Epoch 127/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1915 - accuracy: 0.9328 - val_loss: 0.4548 - val_accuracy: 0.8737\n",
            "Epoch 128/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1939 - accuracy: 0.9322 - val_loss: 0.4080 - val_accuracy: 0.8822\n",
            "Epoch 129/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1809 - accuracy: 0.9364 - val_loss: 0.4145 - val_accuracy: 0.8830\n",
            "Epoch 130/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.3372 - accuracy: 0.8874 - val_loss: 0.8058 - val_accuracy: 0.6612\n",
            "Epoch 131/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.3365 - accuracy: 0.8766 - val_loss: 0.3992 - val_accuracy: 0.8786\n",
            "Epoch 132/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1773 - accuracy: 0.9392 - val_loss: 0.4668 - val_accuracy: 0.8735\n",
            "Epoch 133/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1786 - accuracy: 0.9363 - val_loss: 0.4124 - val_accuracy: 0.8766\n",
            "Epoch 134/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1687 - accuracy: 0.9415 - val_loss: 0.4466 - val_accuracy: 0.8808\n",
            "Epoch 135/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1894 - accuracy: 0.9347 - val_loss: 0.4821 - val_accuracy: 0.8431\n",
            "Epoch 136/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1562 - accuracy: 0.9465 - val_loss: 0.4636 - val_accuracy: 0.8686\n",
            "Epoch 137/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.3495 - accuracy: 0.8871 - val_loss: 0.4193 - val_accuracy: 0.8797\n",
            "Epoch 138/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1608 - accuracy: 0.9444 - val_loss: 0.4085 - val_accuracy: 0.8823\n",
            "Epoch 139/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1606 - accuracy: 0.9442 - val_loss: 0.4807 - val_accuracy: 0.8758\n",
            "Epoch 140/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1526 - accuracy: 0.9473 - val_loss: 0.6481 - val_accuracy: 0.8482\n",
            "Epoch 141/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1632 - accuracy: 0.9418 - val_loss: 0.4139 - val_accuracy: 0.8799\n",
            "Epoch 142/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2506 - accuracy: 0.9232 - val_loss: 0.4717 - val_accuracy: 0.8669\n",
            "Epoch 143/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1426 - accuracy: 0.9502 - val_loss: 0.5066 - val_accuracy: 0.8434\n",
            "Epoch 144/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1627 - accuracy: 0.9463 - val_loss: 0.4287 - val_accuracy: 0.8807\n",
            "Epoch 145/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1496 - accuracy: 0.9497 - val_loss: 0.4523 - val_accuracy: 0.8827\n",
            "Epoch 146/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1240 - accuracy: 0.9586 - val_loss: 0.4438 - val_accuracy: 0.8768\n",
            "Epoch 147/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1397 - accuracy: 0.9524 - val_loss: 0.4356 - val_accuracy: 0.8823\n",
            "Epoch 148/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1353 - accuracy: 0.9535 - val_loss: 0.4646 - val_accuracy: 0.8780\n",
            "Epoch 149/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2261 - accuracy: 0.9305 - val_loss: 0.4451 - val_accuracy: 0.8823\n",
            "Epoch 150/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1227 - accuracy: 0.9579 - val_loss: 0.5431 - val_accuracy: 0.8640\n",
            "Epoch 151/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1209 - accuracy: 0.9596 - val_loss: 0.5296 - val_accuracy: 0.8732\n",
            "Epoch 152/200\n",
            "188/188 [==============================] - 2s 10ms/step - loss: 0.1354 - accuracy: 0.9537 - val_loss: 0.4485 - val_accuracy: 0.8840\n",
            "Epoch 153/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2386 - accuracy: 0.9254 - val_loss: 0.6274 - val_accuracy: 0.7775\n",
            "Epoch 154/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1468 - accuracy: 0.9505 - val_loss: 0.5376 - val_accuracy: 0.8674\n",
            "Epoch 155/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1167 - accuracy: 0.9619 - val_loss: 0.6456 - val_accuracy: 0.8568\n",
            "Epoch 156/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1154 - accuracy: 0.9622 - val_loss: 0.4868 - val_accuracy: 0.8833\n",
            "Epoch 157/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2958 - accuracy: 0.9081 - val_loss: 0.5306 - val_accuracy: 0.8276\n",
            "Epoch 158/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1624 - accuracy: 0.9444 - val_loss: 0.4340 - val_accuracy: 0.8846\n",
            "Epoch 159/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1104 - accuracy: 0.9642 - val_loss: 0.4831 - val_accuracy: 0.8720\n",
            "Epoch 160/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0996 - accuracy: 0.9673 - val_loss: 0.5865 - val_accuracy: 0.8334\n",
            "Epoch 161/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2170 - accuracy: 0.9374 - val_loss: 0.4513 - val_accuracy: 0.8855\n",
            "Epoch 162/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0925 - accuracy: 0.9703 - val_loss: 0.5115 - val_accuracy: 0.8778\n",
            "Epoch 163/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0941 - accuracy: 0.9699 - val_loss: 0.4933 - val_accuracy: 0.8742\n",
            "Epoch 164/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0896 - accuracy: 0.9711 - val_loss: 0.5713 - val_accuracy: 0.8705\n",
            "Epoch 165/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1829 - accuracy: 0.9460 - val_loss: 0.4740 - val_accuracy: 0.8820\n",
            "Epoch 166/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0830 - accuracy: 0.9739 - val_loss: 0.5065 - val_accuracy: 0.8818\n",
            "Epoch 167/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1511 - accuracy: 0.9555 - val_loss: 0.4561 - val_accuracy: 0.8828\n",
            "Epoch 168/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0912 - accuracy: 0.9712 - val_loss: 0.6725 - val_accuracy: 0.8210\n",
            "Epoch 169/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0852 - accuracy: 0.9724 - val_loss: 0.5099 - val_accuracy: 0.8829\n",
            "Epoch 170/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0890 - accuracy: 0.9715 - val_loss: 0.4902 - val_accuracy: 0.8848\n",
            "Epoch 171/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1465 - accuracy: 0.9592 - val_loss: 0.4820 - val_accuracy: 0.8853\n",
            "Epoch 172/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0692 - accuracy: 0.9790 - val_loss: 0.5440 - val_accuracy: 0.8812\n",
            "Epoch 173/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0921 - accuracy: 0.9723 - val_loss: 0.5089 - val_accuracy: 0.8858\n",
            "Epoch 174/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1576 - accuracy: 0.9563 - val_loss: 0.5993 - val_accuracy: 0.8605\n",
            "Epoch 175/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0840 - accuracy: 0.9738 - val_loss: 0.7093 - val_accuracy: 0.8578\n",
            "Epoch 176/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0674 - accuracy: 0.9793 - val_loss: 0.5035 - val_accuracy: 0.8827\n",
            "Epoch 177/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1049 - accuracy: 0.9683 - val_loss: 0.5041 - val_accuracy: 0.8852\n",
            "Epoch 178/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0578 - accuracy: 0.9829 - val_loss: 0.5520 - val_accuracy: 0.8792\n",
            "Epoch 179/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1941 - accuracy: 0.9441 - val_loss: 0.5236 - val_accuracy: 0.8627\n",
            "Epoch 180/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0814 - accuracy: 0.9737 - val_loss: 0.5303 - val_accuracy: 0.8860\n",
            "Epoch 181/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0516 - accuracy: 0.9850 - val_loss: 0.5408 - val_accuracy: 0.8850\n",
            "Epoch 182/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0496 - accuracy: 0.9852 - val_loss: 0.5500 - val_accuracy: 0.8859\n",
            "Epoch 183/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0430 - accuracy: 0.9876 - val_loss: 0.5867 - val_accuracy: 0.8840\n",
            "Epoch 184/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0987 - accuracy: 0.9707 - val_loss: 0.5191 - val_accuracy: 0.8831\n",
            "Epoch 185/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2504 - accuracy: 0.9318 - val_loss: 0.5060 - val_accuracy: 0.8808\n",
            "Epoch 186/200\n",
            "188/188 [==============================] - 2s 10ms/step - loss: 0.0507 - accuracy: 0.9853 - val_loss: 0.5321 - val_accuracy: 0.8835\n",
            "Epoch 187/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0620 - accuracy: 0.9819 - val_loss: 0.5291 - val_accuracy: 0.8813\n",
            "Epoch 188/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0439 - accuracy: 0.9870 - val_loss: 0.6633 - val_accuracy: 0.8793\n",
            "Epoch 189/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1583 - accuracy: 0.9556 - val_loss: 0.9538 - val_accuracy: 0.8323\n",
            "Epoch 190/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0755 - accuracy: 0.9759 - val_loss: 0.5274 - val_accuracy: 0.8857\n",
            "Epoch 191/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0397 - accuracy: 0.9886 - val_loss: 0.6599 - val_accuracy: 0.8669\n",
            "Epoch 192/200\n",
            "188/188 [==============================] - 2s 10ms/step - loss: 0.0973 - accuracy: 0.9725 - val_loss: 0.5874 - val_accuracy: 0.8802\n",
            "Epoch 193/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0386 - accuracy: 0.9894 - val_loss: 0.5752 - val_accuracy: 0.8832\n",
            "Epoch 194/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0308 - accuracy: 0.9919 - val_loss: 0.6839 - val_accuracy: 0.8742\n",
            "Epoch 195/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1488 - accuracy: 0.9609 - val_loss: 0.4761 - val_accuracy: 0.8741\n",
            "Epoch 196/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0692 - accuracy: 0.9780 - val_loss: 0.5707 - val_accuracy: 0.8846\n",
            "Epoch 197/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0306 - accuracy: 0.9921 - val_loss: 0.5885 - val_accuracy: 0.8834\n",
            "Epoch 198/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0320 - accuracy: 0.9910 - val_loss: 0.6067 - val_accuracy: 0.8830\n",
            "Epoch 199/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0264 - accuracy: 0.9933 - val_loss: 0.6179 - val_accuracy: 0.8839\n",
            "Epoch 200/200\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.1075 - accuracy: 0.9687 - val_loss: 0.5739 - val_accuracy: 0.8830\n"
          ]
        }
      ],
      "source": [
        "hst = train_models(model_list, x_train_ready, y_train_cat, x_val, y_val, epochs=200)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_hst = []\n",
        "for h in hst:\n",
        "  best = get_best_epoch(h.history)\n",
        "  if best['val_accuracy'] > 0.88:\n",
        "    filtered_hst.append(h)\n",
        "print(len(filtered_hst))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--54929snYSE",
        "outputId": "b0309a20-f0df-4001-f9b9-ad01f340f55b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('roba_strana.npy',hst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5QVmP40-qMg",
        "outputId": "98fdfaf9-a900-4e1a-ead8-808f59d8cc71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ram://8c05f469-9078-4eac-9da9-4496aac4b857/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history=np.load('.npy',allow_pickle='TRUE')\n",
        "len(history)"
      ],
      "metadata": {
        "id": "3PJejVF5-qtg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4b9961b-a7dc-4d73-9a58-aa73c910cc49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_hist(hst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "dM0QEowLLkwU",
        "outputId": "672677da-75e3-4d74-fad6-f5c3aa9d47bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================\n",
            "UNITS:  [784, 512, 256, 128, 64, 32, 30, 20, 10]\n",
            "INIT:  random_normal - OPT: SGD(lr=0.010) - DROP: False - BATCH_NORM: False\n",
            "LAST VAL_ACC:  0.8687499761581421\n",
            "LAST VAL_LOSS:  0.41879019141197205\n",
            "BEST:  {'accuracy': 0.9013749957084656, 'loss': 0.28580376505851746, 'val_accuracy': 0.8687499761581421, 'val_loss': 0.41879019141197205, 'ind': 99}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(x):\n",
        "    x = x.flatten()\n",
        "    return x"
      ],
      "metadata": {
        "id": "4v1L4VlSYUHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train_fold, y_train_fold), (x_test_fold, y_test_fold) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "x_train_fold_ready = [preprocessing(x) for x in x_train_fold]\n",
        "x_test_fold_ready = [preprocessing(x) for x in x_test_fold]\n",
        "\n",
        "x_train_fold_ready = [x/255 for x in x_train_fold_ready]\n",
        "x_test_fold_ready = [x/255 for x in x_test_fold_ready]\n",
        "\n",
        "x_train_fold_ready = np.asarray(x_train_fold_ready)\n",
        "x_test_fold_ready = np.asarray(x_test_fold_ready)\n",
        "\n",
        "n_features = len(x_train_fold_ready[0])\n",
        "print(n_features)\n",
        "num_classes = 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obJXmVGwX-hn",
        "outputId": "b8d232f2-2174-4092-8df9-2be95381adb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_train_full = ndarray_to_PILIM(x_train)\n",
        "\n",
        "set_test_full = ndarray_to_PILIM(x_test)\n",
        "\n",
        "factor = 1.5\n",
        "resize = 64\n",
        "\n",
        "set_train_contr = []\n",
        "set_val_contr = []\n",
        "set_test_contr = []\n",
        "\n",
        "for im in set_train_full:\n",
        "  #im = im.resize((resize, resize), resample=Image.LANCZOS)\n",
        "  enhancer = ImageEnhance.Contrast(im)\n",
        "  im = enhancer.enhance(factor)\n",
        "  set_train_contr.append(im) \n",
        "\n",
        "for im in set_test_full:\n",
        "  #im = im.resize((resize, resize), resample=Image.LANCZOS)\n",
        "  enhancer = ImageEnhance.Contrast(im)\n",
        "  im = enhancer.enhance(factor)\n",
        "  set_test_contr.append(im) \n",
        "\n",
        "x_train_ready = PILIM_to_ndarray(set_train_contr)\n",
        "\n",
        "x_test_ready = PILIM_to_ndarray(set_test_contr)\n",
        "\n",
        "x_train_ready = [preprocessing(x) for x in x_train_ready]\n",
        "\n",
        "x_test_ready = [preprocessing(x) for x in x_test_ready]\n",
        "\n",
        "x_train_ready = [x/255 for x in x_train_ready]\n",
        "\n",
        "x_test_ready = [x/255 for x in x_test_ready]\n",
        "\n",
        "x_train_ready = np.asarray(x_train_ready)\n",
        "\n",
        "x_test_ready = np.asarray(x_test_ready)\n",
        "\n",
        "n_features = len(x_train_ready[0])\n",
        "print(n_features)\n",
        "num_classes = 10\n",
        "\n",
        "#y_train_cat = to_categorical(y_train, num_classes)\n",
        "#y_val_cat = to_categorical(y_val, num_classes)\n",
        "#y_test_cat = to_categorical(y_test, num_classes)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lX89l7lgvNh",
        "outputId": "c7f20828-68d8-489b-ee42-12b7f8c1345e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''inputs = np.concatenate((x_train_fold_ready, x_test_fold_ready), axis=0)\n",
        "targets = np.concatenate((y_train_fold, y_test_fold), axis=0)'''\n",
        "\n",
        "inputs = x_train_ready\n",
        "targets = y_train\n",
        "\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "NUM_FOLD = 10\n",
        "kfold = KFold(n_splits = NUM_FOLD, shuffle = True)"
      ],
      "metadata": {
        "id": "mrN_HcvPX_O5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-fold Cross Validation model evaluation\n",
        "\n",
        "n_units = [30, 20]\n",
        "activation_fs = [\"relu\", \"relu\"]\n",
        "opt  = [Adam()]\n",
        "\n",
        "hst = []\n",
        "\n",
        "fold_no = 1\n",
        "for i in opt:\n",
        "  for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "    model = MLP_definer_drop(n_units, activation_fs, i, 0.2, tf.keras.losses.SparseCategoricalCrossentropy())\n",
        "\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "                        batch_size = 256,\n",
        "                        epochs = 70, \n",
        "                        verbose = 0)\n",
        "\n",
        "\n",
        "    hst.append(history)\n",
        "    # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "  print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQgruM9FYB_7",
        "outputId": "78cc3d5d-b7a8-403c-fc6d-10a3ce075073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 1: loss of 0.3923262655735016; accuracy of 89.85000252723694%\n",
            "Score for fold 2: loss of 0.43553343415260315; accuracy of 89.30000066757202%\n",
            "Score for fold 3: loss of 0.413545161485672; accuracy of 89.83333110809326%\n",
            "Score for fold 4: loss of 0.4242866635322571; accuracy of 89.85000252723694%\n",
            "Score for fold 5: loss of 0.44610628485679626; accuracy of 88.88333439826965%\n",
            "Score for fold 6: loss of 0.4638098180294037; accuracy of 88.95000219345093%\n",
            "Score for fold 7: loss of 0.4022645652294159; accuracy of 89.81666564941406%\n",
            "Score for fold 8: loss of 0.4053109884262085; accuracy of 89.64999914169312%\n",
            "Score for fold 9: loss of 0.41773855686187744; accuracy of 90.36666750907898%\n",
            "Score for fold 10: loss of 0.4763689339160919; accuracy of 88.98333311080933%\n",
            "> Accuracy: 89.54833388328552 (+- 0.46865332915462093)\n",
            "> Loss: 0.42772906720638276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_hist(hst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "PjWEUPfTczcy",
        "outputId": "6763250a-6b73-4230-ba1e-ab7aacf1907a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-83f4d4c13532>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint_hist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-84ff4f438b43>\u001b[0m in \u001b[0;36mprint_hist\u001b[0;34m(hist_list)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mhst\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhist_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_best_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"============================================================================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-df3061de5369>\u001b[0m in \u001b[0;36mget_best_epoch\u001b[0;34m(diz, watch)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwatch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"val_accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0mls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val_accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m       \u001b[0macc_val_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m       \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_val_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc=0\n",
        "loss=0\n",
        "\n",
        "for hist in hst:\n",
        "  result = hist.model.evaluate(x_test_fold_ready, y_test, batch_size=256)\n",
        "  acc = acc + result[1]\n",
        "  loss = loss + result[0]\n",
        "\n",
        "print(acc/10)\n",
        "print(loss/10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuhkgMZjoFHo",
        "outputId": "ff97d9be-ba03-4130-c24b-0465f887a8b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40/40 [==============================] - 0s 4ms/step - loss: 0.3997 - accuracy: 0.8910\n",
            "40/40 [==============================] - 0s 5ms/step - loss: 0.4289 - accuracy: 0.8906\n",
            "40/40 [==============================] - 0s 4ms/step - loss: 0.3962 - accuracy: 0.8931\n",
            "40/40 [==============================] - 0s 4ms/step - loss: 0.4139 - accuracy: 0.8953\n",
            "40/40 [==============================] - 0s 4ms/step - loss: 0.4136 - accuracy: 0.8902\n",
            "40/40 [==============================] - 0s 4ms/step - loss: 0.4163 - accuracy: 0.8864\n",
            "40/40 [==============================] - 0s 4ms/step - loss: 0.3991 - accuracy: 0.8904\n",
            "40/40 [==============================] - 0s 4ms/step - loss: 0.4090 - accuracy: 0.8881\n",
            "40/40 [==============================] - 0s 4ms/step - loss: 0.4411 - accuracy: 0.8879\n",
            "40/40 [==============================] - 0s 4ms/step - loss: 0.4185 - accuracy: 0.8906\n",
            "0.8903600037097931\n",
            "0.41363205313682555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_ready = normalize(flatten(x_test))\n",
        "y_test_cat = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "Cyg7Q1goiD5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model_list[0].evaluate(x_test_ready, y_test_cat, batch_size=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zEEGlij-md5",
        "outputId": "f229fdaf-0634-4982-b489-72c7ee4ce037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000/10000 [==============================] - 37s 4ms/step - loss: 0.4550 - accuracy: 0.8579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgmWyqbd-8s4",
        "outputId": "d57d6c65-7b34-4f48-c0b3-9acdc237f997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4329693913459778, 0.8952000141143799]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "YmI5Ce--xuxC",
        "aoy2nf3FxXVl",
        "D06Ws39pxmZi",
        "wlpnzw5uzQ4K"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}